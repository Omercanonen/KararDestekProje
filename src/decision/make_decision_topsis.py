
import os
import joblib
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, LabelEncoder

BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))  # src klasÃ¶rÃ¼nden yukarÄ± Ã§Ä±k
MODEL_PATH = os.path.join(BASE_DIR, "models", "catboost", "catboost_model.joblib")
TEST_PATH = os.path.join(BASE_DIR, "data", "processed", "test.csv")
OUTPUT_PATH = os.path.join(BASE_DIR, "reports", "recommendations", "topsis_decision.csv")


os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)


def topsis(matrix, weights, impacts):
    # SÃ¼tun varyansÄ± 0 olanlarÄ± ele
    valid_cols = np.where(matrix.std(axis=0) > 1e-9)[0]
    if len(valid_cols) == 0:
        raise ValueError("TÃ¼m sÃ¼tunlar sabit, TOPSIS uygulanamaz.")
    matrix = matrix[:, valid_cols]
    weights = weights[valid_cols]
    impacts = [impacts[i] for i in valid_cols]

    # Normalize et
    denom = np.sqrt((matrix ** 2).sum(axis=0))
    denom[denom == 0] = 1e-9
    norm = matrix / denom
    norm = np.nan_to_num(norm, nan=0.0)

    weighted = norm * weights
    ideal_best = np.where(np.array(impacts) == '+', weighted.max(axis=0), weighted.min(axis=0))
    ideal_worst = np.where(np.array(impacts) == '+', weighted.min(axis=0), weighted.max(axis=0))
    dist_best = np.sqrt(((weighted - ideal_best) ** 2).sum(axis=1))
    dist_worst = np.sqrt(((weighted - ideal_worst) ** 2).sum(axis=1))
    score = dist_worst / (dist_best + dist_worst)
    return np.nan_to_num(score, nan=0.0)


def clean_data(df: pd.DataFrame):
    df = df.fillna("Unknown")
    for col in df.columns:
        if df[col].dtype == "object" or df[col].dtype.name == "category":
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col].astype(str))
    return df


def main():
    print("ğŸ“¥ Model ve test verisi yÃ¼kleniyor...")

    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"âŒ Model bulunamadÄ±: {MODEL_PATH}")

    model = joblib.load(MODEL_PATH)
    df = pd.read_csv(TEST_PATH)
    X = df.drop(columns=["readmit_30"], errors="ignore")

    # ğŸ§¹ Veri temizleme
    X_clean = clean_data(X)

    # ğŸ§© Model sÃ¼tunlarÄ±nÄ± hizala
    model_features = model.feature_names_
    for col in model_features:
        if col not in X_clean.columns:
            X_clean[col] = 0
    X_clean = X_clean[model_features]

    # ğŸ”® Model tahmin olasÄ±lÄ±klarÄ±
    y_pred_proba = model.predict_proba(X_clean)[:, 1]

    # ğŸ” Ã–zellik Ã¶nemleri
    importance = model.get_feature_importance()
    features = X_clean.columns
    imp_df = pd.DataFrame({"feature": features, "importance": importance})
    imp_df = imp_df.sort_values("importance", ascending=False).reset_index(drop=True)

    # En Ã¶nemli 10 Ã¶zelliÄŸi al
    top_features = imp_df.head(10)["feature"].tolist()

    # âŒ ICD sÃ¼tunlarÄ±nÄ± Ã§Ä±kar
    exclude_features = ["diag_1_cat", "diag_2_cat", "diag_3_cat", "diag_1", "diag_2", "diag_3"]
    top_features = [f for f in top_features if f not in exclude_features]

    # âœ… SayÄ±sal sÃ¼tunlara indirgeme
    X_numeric = X_clean[top_features].select_dtypes(include=[np.number])
    top_features = X_numeric.columns.tolist()

    # EÄŸer hiÃ§ sayÄ±sal Ã¶zellik kalmadÄ±ysa hata verme, tÃ¼m skorlarÄ± 0 yap
    if len(top_features) == 0:
        print("âš ï¸  SayÄ±sal Ã¶zellik kalmadÄ±, tÃ¼m skorlar 0 atandÄ±.")
        scores = np.zeros(len(X_numeric))
        risk_level = ["Bilinmiyor"] * len(scores)
    else:
        matrix = X_numeric.to_numpy(dtype=float)
        scaler = MinMaxScaler()
        matrix = scaler.fit_transform(matrix)

        # AÄŸÄ±rlÄ±klarÄ± hizala
        imp_df_filtered = imp_df[imp_df["feature"].isin(top_features)]
        weights = imp_df_filtered["importance"].values
        weights = weights / weights.sum()
        impacts = ["+"] * len(weights)

        # ğŸ§® TOPSIS skorlarÄ±nÄ± hesapla
        scores = topsis(matrix, weights, impacts)

        # EÄŸer skorlar tamamen 0 ise fallback
        if np.all(scores == 0) or np.isnan(scores).all():
            print("âš ï¸  TOPSIS skorlarÄ± geÃ§ersiz (tÃ¼m deÄŸerler 0 veya NaN).")
            risk_level = ["Bilinmiyor"] * len(scores)
        else:
            risk_level = pd.cut(scores, bins=3,
                                labels=["DÃ¼ÅŸÃ¼k Risk", "Orta Risk", "YÃ¼ksek Risk"],
                                duplicates="drop")

    # ğŸ“Š SonuÃ§ tablosu
    output = pd.DataFrame({
        "PatientID": range(len(scores)),
        "TopsisScore": scores,
        "RiskLevel": risk_level,
        "PredictedProb": y_pred_proba
    })

    output.to_csv(OUTPUT_PATH, index=False)
    print(f"âœ… TOPSIS karar analizi tamamlandÄ±.\nğŸ“ Kaydedildi: {OUTPUT_PATH}")
    print(output.head())


if __name__ == "__main__":
    main()
